{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from yucca.functional.preprocessing import preprocess_case_for_training_with_label\n",
    "from batchgenerators.utilities.file_and_folder_operations import subfiles, join, save_pickle, maybe_mkdir_p, save_json\n",
    "from yucca.paths import yucca_raw_data, yucca_preprocessed_data\n",
    "from yucca.utils.loading import read_file_to_nifti_or_np\n",
    "from yucca.functional.preprocessing import preprocess_case_for_training_with_label\n",
    "from yucca.functional.planning import make_plans_file, add_stats_to_plans_post_preprocessing\n",
    "from yucca.training.managers.YuccaManager import YuccaManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some variables that we'll need and create necessary paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_to_nonzero = True\n",
    "allow_missing_modalities = False\n",
    "norm_op = \"volume_wise_znorm\"\n",
    "plans_name = \"demo\"\n",
    "extension = \".nii.gz\"\n",
    "\n",
    "raw_images_dir = join(yucca_raw_data, \"Task001_OASIS/imagesTr\")\n",
    "raw_labels_dir = join(yucca_raw_data, \"Task001_OASIS/labelsTr\")\n",
    "\n",
    "target_dir = join(yucca_preprocessed_data, \"Task001_OASIS\", plans_name)\n",
    "\n",
    "maybe_mkdir_p(target_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make a barebones plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_to_nonzero = True\n",
    "allow_missing_modalities = False\n",
    "norm_op = \"volume_wise_znorm\"\n",
    "plans_name = \"demo\"\n",
    "modalities = (\"MRI\",)\n",
    "task_type = \"segmentation\"\n",
    "\n",
    "plans = make_plans_file(\n",
    "    allow_missing_modalities=allow_missing_modalities,\n",
    "    crop_to_nonzero=crop_to_nonzero,\n",
    "    norm_op=norm_op,\n",
    "    classes=[0, 1],\n",
    "    plans_name=plans_name,\n",
    "    modalities=modalities,\n",
    "    task_type=task_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now preprocess the samples in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [file[: -len(extension)] for file in subfiles(raw_labels_dir, join=False) if not file.startswith(\".\")]\n",
    "\n",
    "for sub in subjects[:5]:\n",
    "    # we'll just do the first 5 images in this demo\n",
    "    # this still assumes raw images are stored in the yucca format images are saved as:\n",
    "    # sub_XXX.ext where XXX is the modality encoding (e.g 000 and 001 if two modalities are present per subject)\n",
    "    images = [\n",
    "        image_path\n",
    "        for image_path in subfiles(raw_images_dir)\n",
    "        if re.search(re.escape(sub) + \"_\" + r\"\\d{3}\" + \".\", os.path.split(image_path)[-1])\n",
    "    ]\n",
    "    images = [read_file_to_nifti_or_np(image) for image in images]\n",
    "    label = read_file_to_nifti_or_np(join(raw_labels_dir, sub + extension))\n",
    "    images, label, image_props = preprocess_case_for_training_with_label(\n",
    "        images=images,\n",
    "        label=label,\n",
    "        normalization_operation=[\"volume_wise_znorm\"],\n",
    "        allow_missing_modalities=False,\n",
    "        enable_cc_analysis=False,\n",
    "        crop_to_nonzero=True,\n",
    "    )\n",
    "    images = np.vstack((np.array(images), np.array(label)[np.newaxis]), dtype=np.float32)\n",
    "\n",
    "    save_path = join(target_dir, sub)\n",
    "    np.save(save_path + \".npy\", images)\n",
    "    save_pickle(image_props, save_path + \".pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some extra metadata to the plans file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plans = add_stats_to_plans_post_preprocessing(plans=plans, directory=target_dir)\n",
    "save_json(plans, join(target_dir, plans_name + \"_plans.json\"), sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a manager (can be replaced by your own training script/class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 425142129\n",
      "INFO:root:Loading plans.json\n",
      "WARNING:root:Reusing already computed split file which was split using the kfold method and parameter 5.\n",
      "INFO:root:Getting patch size based on manual input of: (32, 32)\n",
      "INFO:root:Using batch size: 2 and patch size: (32, 32)\n",
      "INFO:root:Starting a segmentation task\n",
      "INFO:root:Loading Model: 2D TinyUNet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composing Transforms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zcr545/miniconda3/envs/testyucca/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:root:Using 6 workers\n",
      "INFO:root:Using dataset class: <class 'yucca.training.data_loading.YuccaDataset.YuccaTrainDataset'> for train/val and <class 'yucca.training.data_loading.YuccaDataset.YuccaTestDataset'> for inference\n",
      "INFO:root:\n",
      "| module                      | #parameters or shape   | #flops     |\n",
      "|:----------------------------|:-----------------------|:-----------|\n",
      "| model                       | 7.562K                 | 3.437M     |\n",
      "|  in_conv                    |  0.204K                |  0.451M    |\n",
      "|   in_conv.conv1             |   48                   |   0.115M   |\n",
      "|    in_conv.conv1.conv       |    40                  |    73.728K |\n",
      "|    in_conv.conv1.norm       |    8                   |    40.96K  |\n",
      "|   in_conv.conv2             |   0.156K               |   0.336M   |\n",
      "|    in_conv.conv2.conv       |    0.148K              |    0.295M  |\n",
      "|    in_conv.conv2.norm       |    8                   |    40.96K  |\n",
      "|  encoder_conv1              |  0.912K                |  0.483M    |\n",
      "|   encoder_conv1.conv1       |   0.312K               |   0.168M   |\n",
      "|    encoder_conv1.conv1.conv |    0.296K              |    0.147M  |\n",
      "|    encoder_conv1.conv1.norm |    16                  |    20.48K  |\n",
      "|   encoder_conv1.conv2       |   0.6K                 |   0.315M   |\n",
      "|    encoder_conv1.conv2.conv |    0.584K              |    0.295M  |\n",
      "|    encoder_conv1.conv2.norm |    16                  |    20.48K  |\n",
      "|  encoder_conv2              |  3.552K                |  0.463M    |\n",
      "|   encoder_conv2.conv1       |   1.2K                 |   0.158M   |\n",
      "|    encoder_conv2.conv1.conv |    1.168K              |    0.147M  |\n",
      "|    encoder_conv2.conv1.norm |    32                  |    10.24K  |\n",
      "|   encoder_conv2.conv2       |   2.352K               |   0.305M   |\n",
      "|    encoder_conv2.conv2.conv |    2.32K               |    0.295M  |\n",
      "|    encoder_conv2.conv2.norm |    32                  |    10.24K  |\n",
      "|  upsample1                  |  0.52K                 |  65.536K   |\n",
      "|   upsample1.weight          |   (16, 8, 2, 2)        |            |\n",
      "|   upsample1.bias            |   (8,)                 |            |\n",
      "|  decoder_conv1              |  1.776K                |  0.926M    |\n",
      "|   decoder_conv1.conv1       |   1.176K               |   0.61M    |\n",
      "|    decoder_conv1.conv1.conv |    1.16K               |    0.59M   |\n",
      "|    decoder_conv1.conv1.norm |    16                  |    20.48K  |\n",
      "|   decoder_conv1.conv2       |   0.6K                 |   0.315M   |\n",
      "|    decoder_conv1.conv2.conv |    0.584K              |    0.295M  |\n",
      "|    decoder_conv1.conv2.norm |    16                  |    20.48K  |\n",
      "|  upsample2                  |  0.132K                |  65.536K   |\n",
      "|   upsample2.weight          |   (8, 4, 2, 2)         |            |\n",
      "|   upsample2.bias            |   (4,)                 |            |\n",
      "|  decoder_conv2              |  0.456K                |  0.967M    |\n",
      "|   decoder_conv2.conv1       |   0.3K                 |   0.631M   |\n",
      "|    decoder_conv2.conv1.conv |    0.292K              |    0.59M   |\n",
      "|    decoder_conv2.conv1.norm |    8                   |    40.96K  |\n",
      "|   decoder_conv2.conv2       |   0.156K               |   0.336M   |\n",
      "|    decoder_conv2.conv2.conv |    0.148K              |    0.295M  |\n",
      "|    decoder_conv2.conv2.norm |    8                   |    40.96K  |\n",
      "|  out_conv                   |  10                    |  16.384K   |\n",
      "|   out_conv.weight           |   (2, 4, 1, 1)         |            |\n",
      "|   out_conv.bias             |   (2,)                 |            |\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/zcr545/miniconda3/envs/testyucca/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/zcr545/miniconda3/envs/testyucca/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:186: .fit(ckpt_path=\"last\") is set, but there is no last checkpoint available. No checkpoint will be loaded. HINT: Set `ModelCheckpoint(..., save_last=True)`.\n",
      "INFO:root:Setting up data for stage: TrainerFn.FITTING\n",
      "INFO:root:Training on samples: ['/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1001.npy', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1002.npy', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1006.npy', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1007.npy']\n",
      "INFO:root:Validating on samples: ['/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1000.npy']\n",
      "/Users/zcr545/miniconda3/envs/testyucca/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /Users/zcr545/Desktop/Projects/repos/yucca_data/models/Task001_OASIS/TinyUNet__2D/YuccaManager__demo/default/kfold_5_fold_0/version_0/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | train_metrics | MetricCollection | 0     \n",
      "1 | val_metrics   | MetricCollection | 0     \n",
      "2 | model         | TinyUNet         | 7.6 K \n",
      "3 | loss_fn_train | DiceCE           | 0     \n",
      "4 | loss_fn_val   | DiceCE           | 0     \n",
      "---------------------------------------------------\n",
      "7.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.6 K     Total params\n",
      "0.030     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zcr545/miniconda3/envs/testyucca/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting training with data from: /Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zcr545/miniconda3/envs/testyucca/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:33<00:00,  0.06it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:33<00:00,  0.06it/s, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "Manager = YuccaManager(\n",
    "    enable_logging=False,\n",
    "    ckpt_path=None,\n",
    "    model_name=\"TinyUNet\",\n",
    "    model_dimensions=\"2D\",\n",
    "    num_workers=6,\n",
    "    split_idx=0,\n",
    "    step_logging=True,\n",
    "    task=\"Task001_OASIS\",\n",
    "    planner=plans_name,\n",
    "    patch_size=(32, 32),\n",
    "    batch_size=2,\n",
    "    max_epochs=1,\n",
    "    val_batches_per_step=2,\n",
    "    train_batches_per_step=2,\n",
    "    accelerator=\"cpu\",\n",
    ")\n",
    "Manager.run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testyucca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
