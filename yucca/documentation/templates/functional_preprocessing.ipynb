{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from yucca.functional.preprocessing import preprocess_case_for_training_with_label\n",
    "from batchgenerators.utilities.file_and_folder_operations import subfiles, join, save_pickle, maybe_mkdir_p, save_json\n",
    "from yucca.paths import yucca_raw_data, yucca_preprocessed_data\n",
    "from yucca.functional.utils.loading import read_file_to_nifti_or_np\n",
    "from yucca.functional.preprocessing import (\n",
    "    preprocess_case_for_training_with_label,\n",
    "    preprocess_case_for_training_without_label,\n",
    "    preprocess_case_for_inference,\n",
    ")\n",
    "from yucca.functional.planning import make_plans_file, add_stats_to_plans_post_preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some variables that we'll need and create necessary paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"Task001_OASIS\"\n",
    "crop_to_nonzero = True\n",
    "allow_missing_modalities = False\n",
    "norm_op = \"volume_wise_znorm\"\n",
    "plans_name = \"demo\"\n",
    "modalities = (\"MRI\",)\n",
    "task_type = \"segmentation\"\n",
    "extension = \".nii.gz\"\n",
    "\n",
    "raw_images_dir = join(yucca_raw_data, task, \"imagesTr\")\n",
    "raw_labels_dir = join(yucca_raw_data, task, \"labelsTr\")\n",
    "test_raw_images_dir = join(yucca_raw_data, task, \"imagesTs\")\n",
    "\n",
    "\n",
    "target_dir = join(yucca_preprocessed_data, task, plans_name)\n",
    "test_target_dir = join(yucca_preprocessed_data, task + \"_test\", plans_name)\n",
    "\n",
    "\n",
    "maybe_mkdir_p(target_dir)\n",
    "maybe_mkdir_p(test_target_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make a barebones plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plans = make_plans_file(\n",
    "    allow_missing_modalities=allow_missing_modalities,\n",
    "    crop_to_nonzero=crop_to_nonzero,\n",
    "    norm_op=norm_op,\n",
    "    classes=[0, 1, 2],\n",
    "    plans_name=plans_name,\n",
    "    modalities=modalities,\n",
    "    task_type=task_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now preprocess the samples in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [file[: -len(extension)] for file in subfiles(raw_labels_dir, join=False) if not file.startswith(\".\")]\n",
    "\n",
    "for sub in subjects:\n",
    "    # we'll just do the first 5 images in this demo\n",
    "    # this still assumes raw images are stored in the yucca format images are saved as:\n",
    "    # sub_XXX.ext where XXX is the modality encoding (e.g 000 and 001 if two modalities are present per subject)\n",
    "    images = [\n",
    "        image_path\n",
    "        for image_path in subfiles(raw_images_dir)\n",
    "        if re.search(re.escape(sub) + \"_\" + r\"\\d{3}\" + \".\", os.path.split(image_path)[-1])\n",
    "    ]\n",
    "    images = [read_file_to_nifti_or_np(image) for image in images]\n",
    "    label = read_file_to_nifti_or_np(join(raw_labels_dir, sub + extension))\n",
    "    images, label, image_props = preprocess_case_for_training_with_label(\n",
    "        images=images,\n",
    "        label=label,\n",
    "        normalization_operation=[\"volume_wise_znorm\"],\n",
    "        allow_missing_modalities=False,\n",
    "        enable_cc_analysis=False,\n",
    "        crop_to_nonzero=True,\n",
    "    )\n",
    "    images = np.vstack((np.array(images), np.array(label)[np.newaxis]), dtype=np.float32)\n",
    "\n",
    "    save_path = join(target_dir, sub)\n",
    "    np.save(save_path + \".npy\", images)\n",
    "    save_pickle(image_props, save_path + \".pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some extra metadata to the plans file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plans = add_stats_to_plans_post_preprocessing(plans=plans, directory=target_dir)\n",
    "save_json(plans, join(target_dir, plans_name + \"_plans.json\"), sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's preprocess the test data to make sure it's preprocessed identically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 132, 175, 131])\n",
      "torch.Size([1, 1, 143, 181, 136])\n",
      "torch.Size([1, 1, 142, 183, 134])\n",
      "torch.Size([1, 1, 129, 161, 127])\n",
      "torch.Size([1, 1, 138, 160, 130])\n",
      "torch.Size([1, 1, 132, 175, 132])\n",
      "torch.Size([1, 1, 143, 180, 137])\n",
      "torch.Size([1, 1, 142, 183, 136])\n",
      "torch.Size([1, 1, 130, 159, 127])\n",
      "torch.Size([1, 1, 137, 163, 131])\n",
      "torch.Size([1, 1, 132, 182, 140])\n",
      "torch.Size([1, 1, 136, 175, 137])\n",
      "torch.Size([1, 1, 130, 164, 123])\n",
      "torch.Size([1, 1, 131, 173, 128])\n",
      "torch.Size([1, 1, 135, 174, 136])\n",
      "torch.Size([1, 1, 140, 173, 138])\n",
      "torch.Size([1, 1, 140, 160, 133])\n",
      "torch.Size([1, 1, 134, 159, 121])\n",
      "torch.Size([1, 1, 136, 177, 132])\n",
      "torch.Size([1, 1, 125, 157, 120])\n"
     ]
    }
   ],
   "source": [
    "subjects = [file[: -len(\"_000\" + extension)] for file in subfiles(test_raw_images_dir, join=False) if not file.startswith(\".\")]\n",
    "\n",
    "for sub in subjects:\n",
    "    # we'll just do the first 5 images in this demo\n",
    "    # this still assumes raw images are stored in the yucca format images are saved as:\n",
    "    # sub_XXX.ext where XXX is the modality encoding (e.g 000 and 001 if two modalities are present per subject)\n",
    "    images = [\n",
    "        image_path\n",
    "        for image_path in subfiles(test_raw_images_dir)\n",
    "        if re.search(re.escape(sub) + \"_\" + r\"\\d{3}\" + \".\", os.path.split(image_path)[-1])\n",
    "    ]\n",
    "    images, image_props = preprocess_case_for_inference(\n",
    "        crop_to_nonzero=plans[\"crop_to_nonzero\"],\n",
    "        keep_aspect_ratio=plans[\"keep_aspect_ratio_when_using_target_size\"],\n",
    "        images=images,\n",
    "        intensities=None,\n",
    "        normalization_scheme=[\"volume_wise_znorm\"],\n",
    "        patch_size=(32, 32),\n",
    "        target_size=plans[\"target_size\"],\n",
    "        target_spacing=plans[\"target_spacing\"],\n",
    "        target_orientation=plans[\"target_coordinate_system\"],\n",
    "        transpose_forward=plans[\"transpose_forward\"],\n",
    "    )\n",
    "    # add channel dimension so they're stacked as (b, h, w, d) rather than (h * 2, w, d)\n",
    "    save_path = join(test_target_dir, sub)\n",
    "    torch.save(images, save_path + \".pt\")\n",
    "    save_pickle(image_props, save_path + \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(plans, join(test_target_dir, plans_name + \"_plans.json\"), sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testyucca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
