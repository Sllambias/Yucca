{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yucca.pipeline.managers.YuccaManager import YuccaManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some variables that we'll need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plans_name = \"demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a manager (can be replaced by your own training script/class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 802151415\n",
      "INFO:root:Loading plans.json\n",
      "WARNING:root:Reusing already computed split file which was split using the kfold method and parameter 5.\n",
      "INFO:root:Getting patch size based on manual input of: (32, 32)\n",
      "INFO:root:Using batch size: 2 and patch size: (32, 32)\n",
      "INFO:root:YuccaLightningModule initialized with the following config: {'continue_from_most_recent': True, 'experiment': 'default', 'manager_name': 'YuccaManager', 'model_dimensions': '2D', 'model_name': 'TinyUNet', 'patch_based_training': True, 'planner_name': 'demo', 'task': 'Task001_OASIS', 'split_idx': 0, 'split_method': 'kfold', 'split_param': 5, 'plans_path': '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/demo_plans.json', 'save_dir': '/Users/zcr545/Desktop/Projects/repos/yucca_data/models/Task001_OASIS/TinyUNet__2D/YuccaManager__demo/default/kfold_5_fold_0', 'train_data_dir': '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo', 'version_dir': '/Users/zcr545/Desktop/Projects/repos/yucca_data/models/Task001_OASIS/TinyUNet__2D/YuccaManager__demo/default/kfold_5_fold_0/version_0', 'version': 0, 'base_experiment': 'default', 'ckpt_path': None, 'ckpt_patch_size': None, 'ckpt_seed': None, 'ckpt_plans': None, 'ckpt_version_dir': None, 'ckpt_wandb_id': None, 'seed': 802151415, 'allow_missing_modalities': False, 'image_extension': 'nii.gz', 'num_classes': 3, 'plans': {'target_coordinate_system': 'RAS', 'preprocessor': '', 'crop_to_nonzero': True, 'normalization_scheme': ['volume_wise_znorm'], 'num_classes': 3, 'num_modalities': 1, 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'keep_aspect_ratio_when_using_target_size': True, 'target_size': None, 'target_spacing': [1.0, 1.0, 1.0], 'task_type': 'segmentation', 'dataset_properties': {}, 'plans_name': 'demo', 'suggested_dimensionality': '3D', 'allow_missing_modalities': False, 'n_foreground_locs': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'new_mean_size': [135, 177, 131], 'new_min_size': [127, 164, 124], 'new_max_size': [148, 186, 136], 'mean_cc_size': 0, 'max_cc_size': 0, 'min_cc_size': 0, 'mean_n_cc': 0, 'max_n_cc': 0, 'min_n_cc': 0}, 'task_type': 'segmentation', 'regions_in_order': None, 'regions_labeled': None, 'use_label_regions': False, 'batch_size': 2, 'patch_size': (32, 32), 'num_modalities': 1, 'wandb_id': 'None', 'augmentation_parameters': {'deep_supervision': False, 'pre_aug_patch_size': (45, 45), 'random_crop': False, 'cval': 'min', 'mask_image_for_reconstruction': False, 'patch_size': (32, 32), 'skip_label': False, 'label_dtype': <class 'int'>, 'copy_image_to_label': False, 'convert_labels_to_regions': False, 'additive_noise_p_per_sample': 0.2, 'additive_noise_mean': (0.0, 0.0), 'additive_noise_sigma': (0.001, 0.0001), 'biasfield_p_per_sample': 0.33, 'blurring_p_per_sample': 0.2, 'blurring_sigma': (0.0, 1.0), 'blurring_p_per_channel': 0.5, 'elastic_deform_p_per_sample': 0.33, 'elastic_deform_alpha': (200, 600), 'elastic_deform_sigma': (20, 30), 'gamma_p_per_sample': 0.2, 'gamma_p_invert_image': 0.05, 'gamma_range': (0.5, 2.0), 'gibbs_ringing_p_per_sample': 0.2, 'gibbs_ringing_cut_freq': (96, 129), 'gibbs_ringing_axes': (0, 2), 'mask_ratio': 0.5, 'mirror_p_per_sample': 0.0, 'mirror_p_per_axis': 0.33, 'mirror_axes': (0, 1), 'motion_ghosting_p_per_sample': 0.2, 'motion_ghosting_alpha': (0.85, 0.95), 'motion_ghosting_num_reps': (2, 11), 'motion_ghosting_axes': (0, 2), 'multiplicative_noise_p_per_sample': 0.2, 'multiplicative_noise_mean': (0, 0), 'multiplicative_noise_sigma': (0.001, 0.0001), 'rotation_p_per_sample': 0.2, 'rotation_p_per_axis': 0.66, 'rotation_x': (-30.0, 30.0), 'rotation_y': (-0.0, 0.0), 'rotation_z': (-0.0, 0.0), 'scale_p_per_sample': 0.2, 'scale_factor': (0.9, 1.1), 'simulate_lowres_p_per_sample': 0.2, 'simulate_lowres_p_per_channel': 0.5, 'simulate_lowres_p_per_axis': 0.33, 'simulate_lowres_zoom_range': (0.5, 1.0)}}\n",
      "INFO:root:Deep Supervision Enabled: False\n",
      "INFO:root:Loading Model: 2D TinyUNet\n",
      "INFO:root:Using 4 workers\n",
      "INFO:root:Using dataset class: <class 'yucca.data.datasets.YuccaDataset.YuccaTrainDataset'> for train/val and <class 'yucca.data.datasets.YuccaDataset.YuccaTestDataset'> for inference\n",
      "INFO:root:\n",
      "| module                      | #parameters or shape   | #flops     |\n",
      "|:----------------------------|:-----------------------|:-----------|\n",
      "| model                       | 7.567K                 | 3.445M     |\n",
      "|  in_conv                    |  0.204K                |  0.451M    |\n",
      "|   in_conv.conv1             |   48                   |   0.115M   |\n",
      "|    in_conv.conv1.conv       |    40                  |    73.728K |\n",
      "|    in_conv.conv1.norm       |    8                   |    40.96K  |\n",
      "|   in_conv.conv2             |   0.156K               |   0.336M   |\n",
      "|    in_conv.conv2.conv       |    0.148K              |    0.295M  |\n",
      "|    in_conv.conv2.norm       |    8                   |    40.96K  |\n",
      "|  encoder_conv1              |  0.912K                |  0.483M    |\n",
      "|   encoder_conv1.conv1       |   0.312K               |   0.168M   |\n",
      "|    encoder_conv1.conv1.conv |    0.296K              |    0.147M  |\n",
      "|    encoder_conv1.conv1.norm |    16                  |    20.48K  |\n",
      "|   encoder_conv1.conv2       |   0.6K                 |   0.315M   |\n",
      "|    encoder_conv1.conv2.conv |    0.584K              |    0.295M  |\n",
      "|    encoder_conv1.conv2.norm |    16                  |    20.48K  |\n",
      "|  encoder_conv2              |  3.552K                |  0.463M    |\n",
      "|   encoder_conv2.conv1       |   1.2K                 |   0.158M   |\n",
      "|    encoder_conv2.conv1.conv |    1.168K              |    0.147M  |\n",
      "|    encoder_conv2.conv1.norm |    32                  |    10.24K  |\n",
      "|   encoder_conv2.conv2       |   2.352K               |   0.305M   |\n",
      "|    encoder_conv2.conv2.conv |    2.32K               |    0.295M  |\n",
      "|    encoder_conv2.conv2.norm |    32                  |    10.24K  |\n",
      "|  upsample1                  |  0.52K                 |  65.536K   |\n",
      "|   upsample1.weight          |   (16, 8, 2, 2)        |            |\n",
      "|   upsample1.bias            |   (8,)                 |            |\n",
      "|  decoder_conv1              |  1.776K                |  0.926M    |\n",
      "|   decoder_conv1.conv1       |   1.176K               |   0.61M    |\n",
      "|    decoder_conv1.conv1.conv |    1.16K               |    0.59M   |\n",
      "|    decoder_conv1.conv1.norm |    16                  |    20.48K  |\n",
      "|   decoder_conv1.conv2       |   0.6K                 |   0.315M   |\n",
      "|    decoder_conv1.conv2.conv |    0.584K              |    0.295M  |\n",
      "|    decoder_conv1.conv2.norm |    16                  |    20.48K  |\n",
      "|  upsample2                  |  0.132K                |  65.536K   |\n",
      "|   upsample2.weight          |   (8, 4, 2, 2)         |            |\n",
      "|   upsample2.bias            |   (4,)                 |            |\n",
      "|  decoder_conv2              |  0.456K                |  0.967M    |\n",
      "|   decoder_conv2.conv1       |   0.3K                 |   0.631M   |\n",
      "|    decoder_conv2.conv1.conv |    0.292K              |    0.59M   |\n",
      "|    decoder_conv2.conv1.norm |    8                   |    40.96K  |\n",
      "|   decoder_conv2.conv2       |   0.156K               |   0.336M   |\n",
      "|    decoder_conv2.conv2.conv |    0.148K              |    0.295M  |\n",
      "|    decoder_conv2.conv2.norm |    8                   |    40.96K  |\n",
      "|  out_conv                   |  15                    |  24.576K   |\n",
      "|   out_conv.weight           |   (3, 4, 1, 1)         |            |\n",
      "|   out_conv.bias             |   (3,)                 |            |\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/zcr545/miniconda3/envs/yuccaenv/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/zcr545/miniconda3/envs/yuccaenv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:186: .fit(ckpt_path=\"last\") is set, but there is no last checkpoint available. No checkpoint will be loaded. HINT: Set `ModelCheckpoint(..., save_last=True)`.\n",
      "INFO:root:Setting up data for stage: TrainerFn.FITTING\n",
      "INFO:root:Training on samples: ['/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1001', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1006', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1007', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1008', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1010', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1011', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1012', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1013', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1014', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1015', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1017', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1036']\n",
      "INFO:root:Validating on samples: ['/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1000', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1002', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1009']\n",
      "/Users/zcr545/miniconda3/envs/yuccaenv/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /Users/zcr545/Desktop/Projects/repos/yucca_data/models/Task001_OASIS/TinyUNet__2D/YuccaManager__demo/default/kfold_5_fold_0/version_0/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | train_metrics | MetricCollection | 0     \n",
      "1 | val_metrics   | MetricCollection | 0     \n",
      "2 | model         | TinyUNet         | 7.6 K \n",
      "3 | loss_fn_train | DiceCE           | 0     \n",
      "4 | loss_fn_val   | DiceCE           | 0     \n",
      "---------------------------------------------------\n",
      "7.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.6 K     Total params\n",
      "0.030     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composing Transforms\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zcr545/miniconda3/envs/yuccaenv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]torch.Size([2, 3, 32, 32]) torch.Size([2, 32, 32])\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 123.41it/s]torch.Size([2, 3, 32, 32]) torch.Size([2, 32, 32])\n",
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting training with data from: /Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zcr545/miniconda3/envs/yuccaenv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] torch.Size([2, 3, 32, 32]) torch.Size([2, 32, 32])\n",
      "Epoch 0:  50%|█████     | 1/2 [00:03<00:03,  0.26it/s, v_num=0]torch.Size([2, 3, 32, 32]) torch.Size([2, 32, 32])\n",
      "Epoch 0: 100%|██████████| 2/2 [00:03<00:00,  0.52it/s, v_num=0]torch.Size([2, 3, 32, 32]) torch.Size([2, 32, 32])\n",
      "torch.Size([2, 3, 32, 32]) torch.Size([2, 32, 32])\n",
      "Epoch 1:   0%|          | 0/2 [00:00<?, ?it/s, v_num=0]        torch.Size([2, 3, 32, 32]) torch.Size([2, 32, 32])\n",
      "Epoch 1:  50%|█████     | 1/2 [00:15<00:15,  0.07it/s, v_num=0]torch.Size([2, 3, 32, 32]) torch.Size([2, 32, 32])\n",
      "Epoch 1: 100%|██████████| 2/2 [00:15<00:00,  0.13it/s, v_num=0]torch.Size([2, 3, 32, 32]) torch.Size([2, 32, 32])\n",
      "torch.Size([2, 3, 32, 32]) torch.Size([2, 32, 32])\n",
      "Epoch 1: 100%|██████████| 2/2 [00:35<00:00,  0.06it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2/2 [00:35<00:00,  0.06it/s, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "Manager = YuccaManager(\n",
    "    enable_logging=False,\n",
    "    ckpt_path=None,\n",
    "    model_name=\"TinyUNet\",\n",
    "    model_dimensions=\"2D\",\n",
    "    num_workers=4,\n",
    "    split_idx=0,\n",
    "    step_logging=True,\n",
    "    task=\"Task001_OASIS\",\n",
    "    planner=plans_name,\n",
    "    patch_size=(32, 32),\n",
    "    batch_size=2,\n",
    "    max_epochs=2,\n",
    "    val_batches_per_step=2,\n",
    "    train_batches_per_step=2,\n",
    "    accelerator=\"cpu\",\n",
    ")\n",
    "Manager.run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuccaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
