{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import os\n",
    "from batchgenerators.utilities.file_and_folder_operations import load_json\n",
    "from yucca.pipeline.managers.YuccaManager import YuccaManager\n",
    "from yucca.paths import yucca_raw_data, yucca_preprocessed_data, yucca_models\n",
    "from yucca.pipeline.configuration.configure_task import TaskConfig\n",
    "from yucca.pipeline.configuration.configure_paths import get_path_config\n",
    "from yucca.pipeline.configuration.configure_callbacks import get_callback_config\n",
    "from yucca.pipeline.configuration.split_data import get_split_config\n",
    "from yucca.pipeline.configuration.configure_input_dims import InputDimensionsConfig\n",
    "from yucca.modules.data.augmentation.YuccaAugmentationComposer import YuccaAugmentationComposer\n",
    "from yucca.modules.data.augmentation.augmentation_presets import generic\n",
    "from yucca.modules.lightning_modules.YuccaLightningModule import YuccaLightningModule\n",
    "from yucca.modules.data.data_modules.YuccaDataModule import YuccaDataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some variables that we'll need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\": 2,\n",
    "    \"dims\": \"2D\",\n",
    "    \"deep_supervision\": False,\n",
    "    \"experiment\": \"default\",\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"loss_fn\": \"DiceCE\",\n",
    "    \"model_name\": \"TinyUNet\",\n",
    "    \"momentum\": 0.99,\n",
    "    \"num_classes\": 3,\n",
    "    \"num_modalities\": 1,\n",
    "    \"patch_size\": (32, 32),\n",
    "    \"plans_name\": \"demo\",\n",
    "    \"plans\": None,\n",
    "    \"split_idx\": 0,\n",
    "    \"split_method\": \"kfold\",\n",
    "    \"split_param\": 5,\n",
    "    \"task\": \"Task001_OASIS\",\n",
    "    \"task_type\": \"segmentation\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Reusing already computed split file which was split using the kfold method and parameter 5.\n",
      "/Users/zcr545/miniconda3/envs/yuccaenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composing Transforms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:YuccaLightningModule initialized\n",
      "INFO:root:Deep Supervision Enabled: False\n",
      "INFO:root:Using 9 workers\n",
      "INFO:root:Using dataset class: <class 'yucca.modules.data.datasets.YuccaDataset.YuccaTrainDataset'> for train/val and <class 'yucca.modules.data.datasets.YuccaDataset.YuccaTestDataset'> for inference\n"
     ]
    }
   ],
   "source": [
    "input_dims_config = InputDimensionsConfig(\n",
    "    batch_size=config.get(\"batch_size\"), patch_size=config.get(\"patch_size\"), num_modalities=config.get(\"num_modalitites\")\n",
    ")\n",
    "task_config = TaskConfig(\n",
    "    task=config.get(\"task\"),\n",
    "    continue_from_most_recent=True,\n",
    "    experiment=config.get(\"experiment\"),\n",
    "    manager_name=\"\",\n",
    "    model_dimensions=config.get(\"dims\"),\n",
    "    model_name=config.get(\"model_name\"),\n",
    "    patch_based_training=True,\n",
    "    planner_name=config.get(\"plans_name\"),\n",
    "    split_idx=config.get(\"split_idx\"),\n",
    "    split_method=config.get(\"split_method\"),\n",
    "    split_param=config.get(\"split_param\"),\n",
    ")\n",
    "\n",
    "path_config = get_path_config(task_config=task_config)\n",
    "\n",
    "split_config = get_split_config(method=task_config.split_method, param=task_config.split_param, path_config=path_config)\n",
    "\n",
    "callback_config = get_callback_config(\n",
    "    save_dir=path_config.save_dir,\n",
    "    version_dir=path_config.version_dir,\n",
    "    experiment=task_config.experiment,\n",
    "    version=path_config.version,\n",
    "    enable_logging=False,\n",
    ")\n",
    "\n",
    "augmenter = YuccaAugmentationComposer(\n",
    "    deep_supervision=config.get(\"deep_supervision\"),\n",
    "    patch_size=input_dims_config.patch_size,\n",
    "    is_2D=True if config.get(\"dims\") == \"2D\" else False,\n",
    "    parameter_dict=generic,\n",
    "    task_type_preset=config.get(\"task_type\"),\n",
    ")\n",
    "\n",
    "\n",
    "model_module = YuccaLightningModule(\n",
    "    config=config | task_config.lm_hparams() | path_config.lm_hparams() | callback_config.lm_hparams(),\n",
    "    deep_supervision=config.get(\"deep_supervision\"),\n",
    "    optimizer_kwargs={\"learning_rate\": config.get(\"learning_rate\"), \"momentum\": config.get(\"momentum\")},\n",
    "    loss_fn=config.get(\"loss_fn\"),\n",
    ")\n",
    "\n",
    "data_module = YuccaDataModule(\n",
    "    batch_size=input_dims_config.batch_size,\n",
    "    patch_size=input_dims_config.patch_size,\n",
    "    composed_train_transforms=augmenter.train_transforms,\n",
    "    composed_val_transforms=augmenter.val_transforms,\n",
    "    train_data_dir=path_config.train_data_dir,\n",
    "    split_idx=task_config.split_idx,\n",
    "    splits_config=split_config,\n",
    "    task_type=config.get(\"task_type\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/zcr545/miniconda3/envs/yuccaenv/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/zcr545/miniconda3/envs/yuccaenv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:186: .fit(ckpt_path=\"last\") is set, but there is no last checkpoint available. No checkpoint will be loaded. HINT: Set `ModelCheckpoint(..., save_last=True)`.\n",
      "INFO:root:Setting up data for stage: TrainerFn.FITTING\n",
      "INFO:root:Training on samples: ['/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1000', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1001', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1002', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1008', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1009', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1010', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1011', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1012', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1013', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1014', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1015', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1036']\n",
      "INFO:root:Validating on samples: ['/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1006', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1007', '/Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo/OASIS_1017']\n",
      "/Users/zcr545/miniconda3/envs/yuccaenv/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /Users/zcr545/Desktop/Projects/repos/yucca_data/models/Task001_OASIS/TinyUNet__2D/__demo/default/kfold_5_fold_0/version_0/checkpoints exists and is not empty.\n",
      "INFO:root:Loading Model: 2D TinyUNet\n",
      "INFO:root:\n",
      "| module                      | #parameters or shape   | #flops     |\n",
      "|:----------------------------|:-----------------------|:-----------|\n",
      "| model                       | 7.567K                 | 3.445M     |\n",
      "|  in_conv                    |  0.204K                |  0.451M    |\n",
      "|   in_conv.conv1             |   48                   |   0.115M   |\n",
      "|    in_conv.conv1.conv       |    40                  |    73.728K |\n",
      "|    in_conv.conv1.norm       |    8                   |    40.96K  |\n",
      "|   in_conv.conv2             |   0.156K               |   0.336M   |\n",
      "|    in_conv.conv2.conv       |    0.148K              |    0.295M  |\n",
      "|    in_conv.conv2.norm       |    8                   |    40.96K  |\n",
      "|  encoder_conv1              |  0.912K                |  0.483M    |\n",
      "|   encoder_conv1.conv1       |   0.312K               |   0.168M   |\n",
      "|    encoder_conv1.conv1.conv |    0.296K              |    0.147M  |\n",
      "|    encoder_conv1.conv1.norm |    16                  |    20.48K  |\n",
      "|   encoder_conv1.conv2       |   0.6K                 |   0.315M   |\n",
      "|    encoder_conv1.conv2.conv |    0.584K              |    0.295M  |\n",
      "|    encoder_conv1.conv2.norm |    16                  |    20.48K  |\n",
      "|  encoder_conv2              |  3.552K                |  0.463M    |\n",
      "|   encoder_conv2.conv1       |   1.2K                 |   0.158M   |\n",
      "|    encoder_conv2.conv1.conv |    1.168K              |    0.147M  |\n",
      "|    encoder_conv2.conv1.norm |    32                  |    10.24K  |\n",
      "|   encoder_conv2.conv2       |   2.352K               |   0.305M   |\n",
      "|    encoder_conv2.conv2.conv |    2.32K               |    0.295M  |\n",
      "|    encoder_conv2.conv2.norm |    32                  |    10.24K  |\n",
      "|  upsample1                  |  0.52K                 |  65.536K   |\n",
      "|   upsample1.weight          |   (16, 8, 2, 2)        |            |\n",
      "|   upsample1.bias            |   (8,)                 |            |\n",
      "|  decoder_conv1              |  1.776K                |  0.926M    |\n",
      "|   decoder_conv1.conv1       |   1.176K               |   0.61M    |\n",
      "|    decoder_conv1.conv1.conv |    1.16K               |    0.59M   |\n",
      "|    decoder_conv1.conv1.norm |    16                  |    20.48K  |\n",
      "|   decoder_conv1.conv2       |   0.6K                 |   0.315M   |\n",
      "|    decoder_conv1.conv2.conv |    0.584K              |    0.295M  |\n",
      "|    decoder_conv1.conv2.norm |    16                  |    20.48K  |\n",
      "|  upsample2                  |  0.132K                |  65.536K   |\n",
      "|   upsample2.weight          |   (8, 4, 2, 2)         |            |\n",
      "|   upsample2.bias            |   (4,)                 |            |\n",
      "|  decoder_conv2              |  0.456K                |  0.967M    |\n",
      "|   decoder_conv2.conv1       |   0.3K                 |   0.631M   |\n",
      "|    decoder_conv2.conv1.conv |    0.292K              |    0.59M   |\n",
      "|    decoder_conv2.conv1.norm |    8                   |    40.96K  |\n",
      "|   decoder_conv2.conv2       |   0.156K               |   0.336M   |\n",
      "|    decoder_conv2.conv2.conv |    0.148K              |    0.295M  |\n",
      "|    decoder_conv2.conv2.norm |    8                   |    40.96K  |\n",
      "|  out_conv                   |  15                    |  24.576K   |\n",
      "|   out_conv.weight           |   (3, 4, 1, 1)         |            |\n",
      "|   out_conv.bias             |   (3,)                 |            |\n",
      "\n",
      "  | Name          | Type     | Params\n",
      "-------------------------------------------\n",
      "0 | model         | TinyUNet | 7.6 K \n",
      "1 | loss_fn_train | DiceCE   | 0     \n",
      "2 | loss_fn_val   | DiceCE   | 0     \n",
      "-------------------------------------------\n",
      "7.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.6 K     Total params\n",
      "0.030     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zcr545/miniconda3/envs/yuccaenv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting training with data from: /Users/zcr545/Desktop/Projects/repos/yucca_data/preprocessed/Task001_OASIS/demo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zcr545/miniconda3/envs/yuccaenv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2/2 [00:35<00:00,  0.06it/s, v_num=0]"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    callbacks=callback_config.callbacks,\n",
    "    default_root_dir=path_config.save_dir,\n",
    "    limit_train_batches=2,\n",
    "    limit_val_batches=2,\n",
    "    log_every_n_steps=2,\n",
    "    logger=callback_config.loggers,\n",
    "    precision=\"32\",\n",
    "    profiler=callback_config.profiler,\n",
    "    enable_progress_bar=True,\n",
    "    max_epochs=2,\n",
    "    accelerator=\"cpu\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(\n",
    "    model=model_module,\n",
    "    datamodule=data_module,\n",
    "    ckpt_path=\"last\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuccaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
